{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b5dcbf-3114-4599-b78f-a76708921fe5",
   "metadata": {},
   "source": [
    "# DocuRAG ‚Äî Document Retrieval‚ÄëAugmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b327b1-e163-4c54-9d78-e77f03c1e47c",
   "metadata": {},
   "source": [
    "**DocuRAG** is a production‚Äëstyle, modular Retrieval‚ÄëAugmented Generation (RAG) system designed to demonstrate how modern LLM applications retrieve, ground, and generate answers over user‚Äësupplied documents ‚Äî without LangChain.\n",
    "\n",
    "It is built to showcase applied ML, NLP, and systems design skills relevant to real‚Äëworld RAG deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cb8f9-ac73-451a-b815-9701d2067f5a",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246bb4d-ded7-41c0-a352-a15cf7ca1fa2",
   "metadata": {},
   "source": [
    "Each step in the RAG pipeline is isolated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e67c02-33c6-48b8-bcbb-6615a35793f9",
   "metadata": {},
   "source": [
    "### Ingestion ‚Üí Extraction ‚Üí Chunking ‚Üí Embedding ‚Üí Retrieval ‚Üí Generation, followed by UI + reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee9682-5734-48e2-be16-c34995632bfc",
   "metadata": {},
   "source": [
    "- `ingestion.py` handles upload/URL\n",
    "\n",
    "- `extraction.py` does per-page cascade + optional OCR\n",
    "\n",
    "- `chunking.py` supports dual chunking + auto\n",
    "\n",
    "- `vectorstore.py` ensures unique temp Chroma dirs\n",
    "\n",
    "- `rag.py` orchestrates indexing + retrieval + summary intent + generation\n",
    "\n",
    "- `ui/gradio_app.py` is a thin UI layer (good practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e5bc2-b8e2-419c-bba1-092e0a128646",
   "metadata": {},
   "source": [
    "DocuRAG enables users to ask natural‚Äëlanguage questions over custom knowledge sources provided via:\n",
    "\n",
    "- üìÇ Local PDF upload\n",
    "- üåê URL ingestion (automatic fetch + processing)\n",
    "  \n",
    "The system retrieves the most relevant document segments using vector similarity search (Chroma) and generates context‚Äëgrounded answers using an LLM, with optional summary‚Äëstyle responses when intent is detected.\n",
    "\n",
    "**Key design goals**:\n",
    "\n",
    "- Clear separation of concerns\n",
    "- Extensibility across models and embeddings\n",
    "- Robust document ingestion (including OCR)\n",
    "- Session‚Äëisolated retrieval to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e5d0e-309d-499c-bbc5-77ae6186def4",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de70c9d-38f5-4ec0-83cd-33a362eeb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai chromadb pymupdf gradio python-dotenv requests nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da19121-abb1-4602-adbf-1d8bd98c75c7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b4d44c-441c-4ed8-b028-83e007c53e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client configured successfully.\n",
      "sk-proj-6-J1Chx ********\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import tempfile\n",
    "import shutil\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load .env / environment variables (Codespaces + HF Spaces compatible)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"‚úÖ OpenAI client configured successfully.\")\n",
    "print(api_key[:15], \"********\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34745a0b-e6c8-41c6-a4bd-de316e35ef56",
   "metadata": {},
   "source": [
    "## Optional OCR dependency (safe import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecddb85b-29b4-467b-9d5c-5eb4dc167cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pdfplumber\n",
    "    PDFPLUMBER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDFPLUMBER_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769d4ff-ac7e-4edc-b8a3-e1c4a53a1d9a",
   "metadata": {},
   "source": [
    "## Sentence tokenizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302d2248-88b8-4d06-a3e5-6edff1ba4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_nltk_resources():\n",
    "    \"\"\"\n",
    "    Ensures sentence tokenizer resources exist.\n",
    "    Avoids punkt/punkt_tab runtime crashes in Codespaces/HF.\n",
    "    \"\"\"\n",
    "    for res in [\"punkt\", \"punkt_tab\"]:\n",
    "        try:\n",
    "            nltk.data.find(f\"tokenizers/{res}\")\n",
    "        except LookupError:\n",
    "            nltk.download(res)\n",
    "\n",
    "ensure_nltk_resources()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def safe_sentence_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sentence splitter with regex fallback if NLTK resources fail.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except Exception:\n",
    "        return re.split(r'(?<=[.!?])\\s+', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0dc2e-8af2-4223-8644-cd5396af491f",
   "metadata": {},
   "source": [
    "# INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8b053-5cc0-44ed-89db-e419170cf2ce",
   "metadata": {},
   "source": [
    "#### Ingestion: Upload OR URL ‚Üí local PDF path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2e27f7-6aa2-4dab-b380-7ed6eeefb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdf(file=None, url: Optional[str] = None) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Ingestion stage:\n",
    "    - If a file is uploaded, return its path.\n",
    "    - If a URL is provided, download to /tmp and return local path.\n",
    "    \n",
    "    Returns:\n",
    "      (local_path, source_name)\n",
    "    \"\"\"\n",
    "    if file is not None:\n",
    "        if not file.name.lower().endswith(\".pdf\"):\n",
    "            raise ValueError(\"Uploaded file is not a PDF.\")\n",
    "        return file.name, os.path.basename(file.name)\n",
    "\n",
    "    if url and url.strip():\n",
    "        try:\n",
    "            resp = requests.get(url.strip(), timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            local_path = f\"/tmp/{uuid.uuid4().hex}.pdf\"\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "\n",
    "            source_name = url.strip().split(\"/\")[-1] or \"downloaded.pdf\"\n",
    "            if not source_name.lower().endswith(\".pdf\"):\n",
    "                source_name += \".pdf\"\n",
    "\n",
    "            return local_path, source_name\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download PDF: {e}\")\n",
    "\n",
    "    raise ValueError(\"Please upload a PDF or provide a PDF URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3a3de-b017-4552-ba29-dca955a5ad85",
   "metadata": {},
   "source": [
    "# EXTRACTION\n",
    "#### Text cleaning (improves extraction + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514cab8c-438e-4283-925a-1fd516b78157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleaning stage:\n",
    "    - Removes hyphen line breaks\n",
    "    - Flattens newlines\n",
    "    - Collapses repeated whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe643a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional TRUE OCR dependencies (pytesseract requires system 'tesseract-ocr' binary)\n",
    "try:\n",
    "    import pytesseract\n",
    "    from PIL import Image\n",
    "    PYTESSERACT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYTESSERACT_AVAILABLE = False\n",
    "\n",
    "def ocr_page_with_tesseract(page: fitz.Page, dpi: int = 200) -> str:\n",
    "    \"\"\"Render a PDF page to an image and run TRUE OCR with Tesseract (if available).\n",
    "\n",
    "    Notes:\n",
    "    - This is only used as a *last resort* when text extraction yields nothing.\n",
    "    - Requires `pytesseract` (python) + `tesseract-ocr` (system package).\n",
    "    \"\"\"\n",
    "    if not PYTESSERACT_AVAILABLE:\n",
    "        return \"\"\n",
    "    try:\n",
    "        pix = page.get_pixmap(dpi=dpi)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        return clean_text(pytesseract.image_to_string(img))\n",
    "    except Exception:\n",
    "        # Keep OCR failure non-fatal; downstream will handle empty extraction.\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cf352-b835-408d-83e3-1fdd0b22ffb1",
   "metadata": {},
   "source": [
    "#### Extraction: PDF ‚Üí per-page text (PyMuPDF, optional OCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4bc9c5-500c-4b10-a6ef-b83f6ff4bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages_with_cascade(file_path: str) -> Tuple[List[str], Dict]:\n",
    "    \"\"\"Extraction stage with per-page extractor cascade.\n",
    "\n",
    "    Cascade per page:\n",
    "      1) PyMuPDF (fitz) text extraction\n",
    "      2) pdfplumber text-layer extraction (if installed)\n",
    "      3) TRUE OCR via pytesseract (if available)\n",
    "\n",
    "    Returns:\n",
    "      pages_text: list[str] cleaned text per page (may contain empty strings)\n",
    "      stats: dict with extraction counts (useful for status/debug)\n",
    "    \"\"\"\n",
    "    pages_text: List[str] = []\n",
    "    stats = {\n",
    "        \"pages_total\": 0,\n",
    "        \"fitz_pages_with_text\": 0,\n",
    "        \"plumber_pages_with_text\": 0,\n",
    "        \"ocr_pages_with_text\": 0,\n",
    "        \"empty_pages\": 0,\n",
    "        \"pytesseract_available\": PYTESSERACT_AVAILABLE,\n",
    "        \"pdfplumber_available\": PDFPLUMBER_AVAILABLE,\n",
    "    }\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    stats[\"pages_total\"] = len(doc)\n",
    "\n",
    "    plumber_pdf = None\n",
    "    if PDFPLUMBER_AVAILABLE:\n",
    "        try:\n",
    "            import pdfplumber  # local import to keep optional dependency truly optional\n",
    "            plumber_pdf = pdfplumber.open(file_path)\n",
    "        except Exception:\n",
    "            plumber_pdf = None\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "\n",
    "        # 1) Primary: fitz\n",
    "        txt = clean_text(page.get_text(\"text\") or \"\")\n",
    "        if txt.strip():\n",
    "            stats[\"fitz_pages_with_text\"] += 1\n",
    "            pages_text.append(txt)\n",
    "            continue\n",
    "\n",
    "        # 2) Secondary: pdfplumber (text layer)\n",
    "        if plumber_pdf is not None:\n",
    "            try:\n",
    "                ptxt = clean_text(plumber_pdf.pages[page_num].extract_text() or \"\")\n",
    "            except Exception:\n",
    "                ptxt = \"\"\n",
    "            if ptxt.strip():\n",
    "                stats[\"plumber_pages_with_text\"] += 1\n",
    "                pages_text.append(ptxt)\n",
    "                continue\n",
    "\n",
    "        # 3) Last resort: TRUE OCR\n",
    "        otxt = ocr_page_with_tesseract(page) if PYTESSERACT_AVAILABLE else \"\"\n",
    "        if otxt.strip():\n",
    "            stats[\"ocr_pages_with_text\"] += 1\n",
    "            pages_text.append(otxt)\n",
    "        else:\n",
    "            stats[\"empty_pages\"] += 1\n",
    "            pages_text.append(\"\")\n",
    "\n",
    "    if plumber_pdf is not None:\n",
    "        try:\n",
    "            plumber_pdf.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "    doc.close()\n",
    "\n",
    "    return pages_text, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1151b-653d-45e3-9ad8-804f2e6c9b8b",
   "metadata": {},
   "source": [
    "# CHUNKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212dda52-bb7e-4c88-9cb0-30fb77cfe27d",
   "metadata": {},
   "source": [
    "#### Smaller, safer chunking with overlap (returns metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1908f167-33e9-4f82-ac29-daa92ebc68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Drop-in chunking implementations (LangChain-free)\n",
    "# We expose 3 strategies:\n",
    "# 1) Word-window (robust for long/noisy/OCR text)\n",
    "# 2) Sentence-based (more readable citations; great for summaries)\n",
    "# 3) Unified controller (\"auto\") that chooses based on page length\n",
    "\n",
    "def chunk_word_window(\n",
    "    text: str,\n",
    "    source_name: str,\n",
    "    page_num: int,\n",
    "    chunk_words: int = 180,\n",
    "    overlap: int = 40\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Fixed-size word window chunking with overlap.\n",
    "\n",
    "    Why: consistent chunk sizes improve embedding stability, especially on OCR/noisy text.\n",
    "    Trade-off: may cut across sentence boundaries.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks: List[Dict] = []\n",
    "\n",
    "    step = max(1, chunk_words - overlap)\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = \" \".join(words[i:i + chunk_words]).strip()\n",
    "        if len(chunk) < 120:\n",
    "            continue\n",
    "        chunks.append({\"source\": source_name, \"page\": page_num + 1, \"text\": chunk})\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_sentence_based(\n",
    "    text: str,\n",
    "    source_name: str,\n",
    "    page_num: int,\n",
    "    max_words: int = 220\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Sentence-preserving chunking.\n",
    "\n",
    "    Why: keeps sentences intact ‚Üí better readability and cleaner citations.\n",
    "    Great for: summaries / contributions / overview questions.\n",
    "    Trade-off: chunk sizes vary; long paragraphs can create larger chunks.\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in safe_sentence_split(text) if len(s.strip()) > 20]\n",
    "    chunks: List[Dict] = []\n",
    "    current: List[str] = []\n",
    "    current_words = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        w = len(sent.split())\n",
    "\n",
    "        # Hard cap extremely long sentences\n",
    "        if w > max_words:\n",
    "            sent = \" \".join(sent.split()[:max_words])\n",
    "            w = len(sent.split())\n",
    "\n",
    "        if current_words + w > max_words and current:\n",
    "            chunks.append({\"source\": source_name, \"page\": page_num + 1, \"text\": \" \".join(current)})\n",
    "            current = []\n",
    "            current_words = 0\n",
    "\n",
    "        current.append(sent)\n",
    "        current_words += w\n",
    "\n",
    "    if current:\n",
    "        chunks.append({\"source\": source_name, \"page\": page_num + 1, \"text\": \" \".join(current)})\n",
    "\n",
    "    # Filter tiny chunks\n",
    "    return [c for c in chunks if len(c[\"text\"]) >= 120]\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    source_name: str,\n",
    "    page_num: int,\n",
    "    mode: str = \"auto\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Unified chunking interface.\n",
    "\n",
    "    mode:\n",
    "      - \"word\"      ‚Üí word-window chunking\n",
    "      - \"sentence\"  ‚Üí sentence/paragraph-preserving\n",
    "      - \"auto\"      ‚Üí choose based on text length (long/OCR-like ‚Üí word)\n",
    "    \"\"\"\n",
    "    mode = (mode or \"auto\").lower().strip()\n",
    "    if mode == \"word\":\n",
    "        return chunk_word_window(text, source_name, page_num)\n",
    "    if mode == \"sentence\":\n",
    "        return chunk_sentence_based(text, source_name, page_num)\n",
    "\n",
    "    # AUTO heuristic: long pages tend to work better with word windows (especially OCR output)\n",
    "    word_count = len((text or \"\").split())\n",
    "    if word_count > 900:\n",
    "        return chunk_word_window(text, source_name, page_num)\n",
    "    return chunk_sentence_based(text, source_name, page_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666dcbc5-7e68-4072-9170-ae8fb60154e6",
   "metadata": {},
   "source": [
    "# EMBEDDING + INDEXING (Chroma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8ba0f-baf0-406a-800e-c89aff4f28c1",
   "metadata": {},
   "source": [
    "#### Session-safe Chroma store (unique temp dir per reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88d5642-7ef0-487e-ab98-33eed3516108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chroma initialized: /tmp/chroma_session_c8935s7j\n"
     ]
    }
   ],
   "source": [
    "SESSION_DIR = None\n",
    "chroma_client = None\n",
    "collection = None\n",
    "\n",
    "def create_vector_store() -> chromadb.api.models.Collection.Collection:\n",
    "    \"\"\"\n",
    "    Embedding/Index stage setup:\n",
    "    Uses a unique temporary directory per session to avoid conflicts/read-only issues.\n",
    "    \"\"\"\n",
    "    global SESSION_DIR, chroma_client, collection\n",
    "\n",
    "    if SESSION_DIR and os.path.exists(SESSION_DIR):\n",
    "        shutil.rmtree(SESSION_DIR, ignore_errors=True)\n",
    "\n",
    "    SESSION_DIR = tempfile.mkdtemp(prefix=\"chroma_session_\")\n",
    "    chroma_client = chromadb.PersistentClient(path=SESSION_DIR)\n",
    "\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key,\n",
    "        model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"pdf_rag_collection\",\n",
    "        embedding_function=emb_fn\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "create_vector_store()\n",
    "print(\"‚úÖ Chroma initialized:\", SESSION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecec1d7-4956-4bca-bd32-05e41fe6f691",
   "metadata": {},
   "source": [
    "#### Indexing: store chunks + metadata in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "693b1a54-2379-46b0-b51a-061e9dc7d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document(file_path: str, source_name: str, chunk_mode: str = \"auto\") -> str:\n",
    "    \"\"\"Embedding/Indexing stage:\n",
    "    - Extract pages with a per-page cascade (fitz ‚Üí pdfplumber ‚Üí OCR)\n",
    "    - Chunk pages using selectable strategies (word-window, sentence-preserving, auto)\n",
    "    - Add to Chroma with collision-safe UUID IDs\n",
    "\n",
    "    Adds a reviewer-friendly debug line to the status:\n",
    "      \"Chunking used: <selected> ‚Üí <resolved modes> | Chunks: N\"\n",
    "    where <resolved modes> is:\n",
    "      - \"word\" or \"sentence\" if a fixed mode is chosen\n",
    "      - \"word/sentence\" (or whichever occurred) if chunk_mode=\"auto\"\n",
    "\n",
    "    Returns a human-readable status string for the UI.\n",
    "    \"\"\"\n",
    "    pages, stats = extract_pages_with_cascade(file_path)\n",
    "\n",
    "    all_chunks: List[Dict] = []\n",
    "    resolved_modes_used = set()\n",
    "    chunk_counts_by_mode = {\"word\": 0, \"sentence\": 0}\n",
    "\n",
    "    for page_num, page_text in enumerate(pages):\n",
    "        if not page_text or not page_text.strip():\n",
    "            continue\n",
    "\n",
    "        # Determine which strategy was effectively used (for reporting)\n",
    "        if chunk_mode == \"auto\":\n",
    "            # Mirror the heuristic used in chunk_text()\n",
    "            resolved = \"word\" if len(page_text.split()) > 900 else \"sentence\"\n",
    "        else:\n",
    "            resolved = chunk_mode\n",
    "\n",
    "        resolved_modes_used.add(resolved)\n",
    "\n",
    "        page_chunks = chunk_text(page_text, source_name, page_num, mode=chunk_mode)\n",
    "        all_chunks.extend(page_chunks)\n",
    "\n",
    "        if resolved in chunk_counts_by_mode:\n",
    "            chunk_counts_by_mode[resolved] += len(page_chunks)\n",
    "\n",
    "    if not all_chunks:\n",
    "        # Clearer, actionable message\n",
    "        if not stats.get(\"pytesseract_available\", False):\n",
    "            return (\n",
    "                \"‚ùå No readable text extracted. This PDF may be image-based. \"\n",
    "                \"Optional OCR is disabled (pytesseract/tesseract not available).\\n\\n\"\n",
    "                f\"Extraction stats: {stats}\"\n",
    "            )\n",
    "        return (\n",
    "            \"‚ùå No readable text extracted even after OCR fallback.\\n\\n\"\n",
    "            f\"Extraction stats: {stats}\"\n",
    "        )\n",
    "\n",
    "    ids = [uuid.uuid4().hex for _ in all_chunks]  # avoids collisions on re-index\n",
    "    collection.add(\n",
    "        documents=[c[\"text\"] for c in all_chunks],\n",
    "        metadatas=[{\"source\": c[\"source\"], \"page\": c[\"page\"], \"text\": c[\"text\"]} for c in all_chunks],\n",
    "        ids=ids,\n",
    "    )\n",
    "\n",
    "    resolved_str = \"/\".join(sorted(resolved_modes_used)) if resolved_modes_used else chunk_mode\n",
    "    chunking_debug = f\"Chunking used: {chunk_mode} ‚Üí {resolved_str} | Chunks: {len(all_chunks)}\"\n",
    "    if chunk_mode == \"auto\":\n",
    "        chunking_debug += f\" (word: {chunk_counts_by_mode['word']}, sentence: {chunk_counts_by_mode['sentence']})\"\n",
    "\n",
    "    return (\n",
    "        f\"‚úÖ Document Indexed Successfully\\n\"\n",
    "        f\"{chunking_debug}\\n\"\n",
    "        f\"Pages: {stats['pages_total']} | fitz: {stats['fitz_pages_with_text']} | \"\n",
    "        f\"pdfplumber: {stats['plumber_pages_with_text']} | OCR: {stats['ocr_pages_with_text']} | \"\n",
    "        f\"empty: {stats['empty_pages']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cacfa7-c371-4b12-9eb7-c7b002a4dee0",
   "metadata": {},
   "source": [
    "# RETRIEVAL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e76b1-9418-4b81-ae5e-8210b7f6bcf9",
   "metadata": {},
   "source": [
    "#### Retrieval: query Chroma (fallback if empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ab791c-be2b-403e-8b7e-a372af521bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 6) -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Retrieval stage:\n",
    "    Query Chroma and return (docs, metas).\n",
    "    \"\"\"\n",
    "    query = (query or \"\").strip()\n",
    "    if not query:\n",
    "        return [], []\n",
    "\n",
    "    results = collection.query(query_texts=[query], n_results=k)\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    metas = results.get(\"metadatas\", [[]])[0]\n",
    "    return docs, metas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b4c8c-4715-4936-90db-f150748f99cc",
   "metadata": {},
   "source": [
    "# GENERATION (LLM) + CITATIONS\n",
    "#### Generate grounded answer; suppress citations if ‚Äúno relevant info‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_summary_intent(query: str) -> bool:\n",
    "    \"\"\"Heuristic intent detector for summary-style questions.\n",
    "\n",
    "    Why:\n",
    "    - Summary/contribution questions need broader context than factual Q&A.\n",
    "    - We can automatically increase Top-K and switch to a summary prompt.\n",
    "    \"\"\"\n",
    "    q = (query or \"\").lower()\n",
    "    triggers = [\n",
    "        \"summarize\", \"summary\", \"overview\", \"main contribution\", \"main contributions\",\n",
    "        \"key contributions\", \"what is this paper about\", \"abstract\", \"tl;dr\", \"tldr\"\n",
    "    ]\n",
    "    return any(t in q for t in triggers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b765907b-6dfe-44fd-a1b4-d397028bb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, docs: List[str], metas: List[Dict], summary_mode: bool = False) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"Generation stage (direct OpenAI; no LangChain).\n",
    "\n",
    "    - If `summary_mode` is True, produce a structured paper-style summary.\n",
    "    - If no docs are retrieved, return a clear 'no relevant information' response with no citations.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"The provided context does not contain relevant information.\", []\n",
    "\n",
    "    per_chunk_cap = 900 if not summary_mode else 1100\n",
    "\n",
    "    context_blocks = []\n",
    "    for d, m in zip(docs, metas):\n",
    "        context_blocks.append(f\"[{m['source']} | Page {m['page']}] {d[:per_chunk_cap]}\")\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    if summary_mode:\n",
    "        prompt = f\"\"\"\n",
    "You are a research assistant.\n",
    "\n",
    "Task: Produce a concise but informative summary of the paper using ONLY the provided context.\n",
    "\n",
    "Output format:\n",
    "- **Problem / Motivation** (1-2 bullets)\n",
    "- **Approach / Method** (2-4 bullets)\n",
    "- **Main Contributions** (3-6 bullets)\n",
    "- **Key Results / Claims** (1-3 bullets)\n",
    "- **Limitations / Open Questions** (1-3 bullets)\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the context.\n",
    "- If something is missing, explicitly say it is missing.\n",
    "- Add citations at the end of each bullet in the format [Source, Page X].\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "You are a research assistant.\n",
    "\n",
    "Rules:\n",
    "- Answer ONLY using the provided context.\n",
    "- If the answer is not in the context, say exactly:\n",
    "  \"The provided context does not contain relevant information.\"\n",
    "- Keep the answer concise (5-8 sentences max).\n",
    "- Add citations where appropriate in the format [Source, Page X].\n",
    "- Do not invent citations.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = resp.choices[0].message.content.strip()\n",
    "\n",
    "        if \"does not contain relevant information\" in answer.lower():\n",
    "            return answer, []\n",
    "\n",
    "        return answer, metas\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå LLM generation failed: {str(e)}\", []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73005dbc-cb31-4e13-9ced-8327efd5a493",
   "metadata": {},
   "source": [
    "#### Citation formatting (300 chars, no mid-word cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de40a6c5-a70e-4068-b518-68e74daf5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources(metas: List[Dict], max_chars: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Formats citations for UI:\n",
    "    - includes source + page + short snippet (no mid-word cuts)\n",
    "    - dedupes repeated items\n",
    "    \"\"\"\n",
    "    if not metas:\n",
    "        return \"No citations.\"\n",
    "\n",
    "    lines = [\"### üìö Sources\"]\n",
    "    seen = set()\n",
    "\n",
    "    for m in metas:\n",
    "        raw = (m.get(\"text\") or \"\").replace(\"\\n\", \" \").strip()\n",
    "        key = (m.get(\"source\"), m.get(\"page\"), raw[:120])\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        if len(raw) > max_chars:\n",
    "            cut = raw.rfind(\" \", 0, max_chars)\n",
    "            cut = cut if cut != -1 else max_chars\n",
    "            raw = raw[:cut].rstrip() + \"...\"\n",
    "\n",
    "        lines.append(f'- **{m.get(\"source\",\"Unknown\")}** (Page {m.get(\"page\",\"?\")}): \"{raw}\"')\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4a848-9502-4d75-b5d5-aeb39e02f907",
   "metadata": {},
   "source": [
    "# UI + Reset\n",
    "#### Reset: clears DB + clears UI fields safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413eb676-6ae4-4597-86ac-aafb5f55e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_all():\n",
    "    \"\"\"\n",
    "    Reset button:\n",
    "    - creates a fresh Chroma session\n",
    "    - returns correct types for Gradio components\n",
    "    \"\"\"\n",
    "    create_vector_store()\n",
    "    return (\n",
    "        \"\",     # answer_output\n",
    "        \"\",     # sources_output\n",
    "        \"\",     # status_box\n",
    "        None,   # file_input\n",
    "        \"\",     # url_input\n",
    "        \"\"      # query_input\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afba7cc-e7af-4876-876f-87bba773f94a",
   "metadata": {},
   "source": [
    "#### Debug formatting helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c59cf786-c419-474d-a7b6-b8c4dafa14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_debug_retrieval(docs: List[str], metas: List[Dict], max_chars: int = 450) -> str:\n",
    "    \"\"\"\n",
    "    Debug panel:\n",
    "    Shows what the retriever returned (top-k) so reviewers can inspect RAG behavior.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No retrieved chunks (empty retrieval result).\"\n",
    "\n",
    "    lines = [\"### üß™ Retrieval Debug (Top-K Chunks)\"]\n",
    "    for i, (d, m) in enumerate(zip(docs, metas), start=1):\n",
    "        raw = (d or \"\").replace(\"\\n\", \" \").strip()\n",
    "        if len(raw) > max_chars:\n",
    "            cut = raw.rfind(\" \", 0, max_chars)\n",
    "            cut = cut if cut != -1 else max_chars\n",
    "            raw = raw[:cut].rstrip() + \"...\"\n",
    "\n",
    "        lines.append(\n",
    "            f\"**{i}. {m.get('source','Unknown')} ‚Äî Page {m.get('page','?')}**\\n\\n\"\n",
    "            f\"> {raw}\\n\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f583268-f4a3-4676-8d0a-617c55cbfe7f",
   "metadata": {},
   "source": [
    "#### Process handler (supports debug toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6cf3d11-8afe-439c-bb62-3344bdfef3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(file, url, query, k, debug_mode, chunk_mode):\n",
    "    \"\"\"Gradio handler.\n",
    "\n",
    "    Behaviors:\n",
    "    - If a file/URL is provided, we reset the session store and re-index the document.\n",
    "    - We auto-detect summary-style questions and widen retrieval (higher Top-K) + switch prompt.\n",
    "    - If retrieval returns no hits, we return a clear answer with *no citations* and update status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not query or not query.strip():\n",
    "            return \"Error: Please type your question here.\", \"\", \"‚ùå Missing question.\", \"\"\n",
    "\n",
    "        status = \"Using existing indexed document.\"\n",
    "\n",
    "        # New doc provided -> reset + index\n",
    "        if file or (url and url.strip()):\n",
    "            create_vector_store()\n",
    "            local_path, source_name = ingest_pdf(file=file, url=url.strip() if url else None)\n",
    "            status = index_document(local_path, source_name, chunk_mode=chunk_mode)\n",
    "\n",
    "        q = query.strip()\n",
    "        summary_mode = is_summary_intent(q)\n",
    "\n",
    "        # Broader retrieval for summaries/contributions\n",
    "        eff_k = max(int(k), 10) if summary_mode else int(k)\n",
    "\n",
    "        docs, metas = retrieve(q, k=eff_k)\n",
    "\n",
    "        debug_text = format_debug_retrieval(docs, metas) if debug_mode else \"\"\n",
    "\n",
    "        if not docs:\n",
    "            status = status + \"\\n‚ö†Ô∏è Retrieval returned 0 relevant chunks. Try increasing Top-K or re-indexing.\"\n",
    "            return \"The provided context does not contain relevant information.\", \"No citations.\", status, debug_text\n",
    "\n",
    "        answer, used = generate_answer(q, docs, metas, summary_mode=summary_mode)\n",
    "        citations = format_sources(used, max_chars=300)\n",
    "\n",
    "        return answer, citations, status, debug_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\", \"‚ùå Failed.\", \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df18fd-e2d3-4143-8dc3-cfde6bd8a95a",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a432c6-074b-4346-b34c-00f8dada51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21346/4031861923.py:1: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft(), title=\"PDF RAG (No LangChain)\") as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* Running on public URL: https://22b606a89c1c569df5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://22b606a89c1c569df5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"PDF RAG (No LangChain)\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"# üìÑ PDF RAG Assistant (No LangChain)\\n\"\n",
    "        \"Upload a PDF or paste a PDF URL. Ask questions and get answers with **source + page** citations.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### üì• Document\")\n",
    "            file_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "            url_input = gr.Textbox(label=\"Or PDF URL\", placeholder=\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
    "\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Retrieval\")\n",
    "            k_slider = gr.Slider(2, 10, value=6, step=1, label=\"Top-K chunks\")\n",
    "            debug_mode = gr.Checkbox(value=False, label=\"Show retrieval debug\")\n",
    "\n",
    "            chunk_mode = gr.Dropdown(\n",
    "                choices=[\"auto\", \"sentence\", \"word\"],\n",
    "                value=\"auto\",\n",
    "                label=\"Chunking strategy\",\n",
    "                info=\"auto=best default ‚Ä¢ sentence=best for summaries ‚Ä¢ word=best for OCR/noisy text\"\n",
    "            )\n",
    "\n",
    "            status_box = gr.Textbox(label=\"Status\", value=\"Ready.\", interactive=False)\n",
    "\n",
    "            clear_btn = gr.Button(\"Clear / Reset session\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### üí¨ Ask\")\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Type your question here\",\n",
    "                placeholder=\"e.g., What is self-attention and why is it useful?\",\n",
    "                lines=2\n",
    "            )\n",
    "            ask_btn = gr.Button(\"Ask\", variant=\"primary\")\n",
    "\n",
    "            gr.Markdown(\"### ‚úÖ Answer\")\n",
    "            answer_output = gr.Markdown()\n",
    "\n",
    "            gr.Markdown(\"### üìö Citations\")\n",
    "            sources_output = gr.Markdown()\n",
    "\n",
    "            with gr.Accordion(\"üß™ Retrieval Debug (Top-K chunks)\", open=False):\n",
    "                debug_output = gr.Markdown(value=\"(Enable 'Show retrieval debug' to display retrieved chunks.)\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        process_input,\n",
    "        inputs=[file_input, url_input, query_input, k_slider, debug_mode, chunk_mode],\n",
    "        outputs=[answer_output, sources_output, status_box, debug_output]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        clear_all,\n",
    "        outputs=[answer_output, sources_output, status_box, file_input, url_input, query_input]\n",
    "    ).then(\n",
    "        lambda: \"(Enable 'Show retrieval debug' to display retrieved chunks.)\",\n",
    "        outputs=debug_output\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8982a39-5a5e-4559-a68f-f21137a3869b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
